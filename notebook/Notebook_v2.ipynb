{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":97258,"sourceType":"competition"},{"sourceId":11388536,"sourceType":"datasetVersion","datasetId":7131662}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:22:29.410413Z","iopub.execute_input":"2025-04-19T15:22:29.410635Z","iopub.status.idle":"2025-04-19T15:22:30.921066Z","shell.execute_reply.started":"2025-04-19T15:22:29.410617Z","shell.execute_reply":"2025-04-19T15:22:30.920011Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cx-agent-dataset/faq.csv\n/kaggle/input/cx-agent-dataset/tickets.csv\n/kaggle/input/cx-agent-dataset/user_info.csv\n/kaggle/input/cx-agent-dataset/payments.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# 1. Introduction and Problem statement\n\nCustomer support is at the heart of user satisfaction, yet it’s often one of the most resource-draining functions in any organization. Whether you’re in property tech, e-commerce, travel, or SaaS — the need for accurate, fast, and scalable support is universal.\n\nWhile the use case we’ll explore is rooted in the property tech industry, the architecture is highly adaptable and can be applied across domains. We’ll focus on modular tooling, smart decision routing, and scalable design choices that help take your project from prototype to production-ready.\n\n\n**Why Now? The Case for Smarter Customer Support**\n\nAs businesses grow, so do customer queries — often faster than our support team can scale. Most companies tackle this by hiring more agents, but that’s rarely sustainable.\n\nEnter chatbots.\n\nModern AI agents, powered by LLMs, aren’t just rule-based bots responding to keywords. With the ability to understand context, fetch relevant information, and respond conversationally, they’re redefining how support can work — handling 60–70% of queries autonomously while escalating only the truly complex cases.","metadata":{}},{"cell_type":"code","source":"#Libraries \n!pip install -q \\\n  langchain \\\n  langchain-huggingface \\\n  langchain_community \\\n  langchainhub \\\n  langchain-groq \\\n  chromadb \\\n  sentence-transformers \\\n  huggingface-hub \\\n  python-dotenv \\\n  langchain-mistralai \\\n  streamlit \\\n  langgraph","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T03:43:08.009163Z","iopub.execute_input":"2025-04-20T03:43:08.009409Z","iopub.status.idle":"2025-04-20T03:45:39.325129Z","shell.execute_reply.started":"2025-04-20T03:43:08.009385Z","shell.execute_reply":"2025-04-20T03:45:39.323555Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.2/145.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.7/126.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.9/433.9 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\ngoogle-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:25:26.357744Z","iopub.execute_input":"2025-04-19T15:25:26.358078Z","iopub.status.idle":"2025-04-19T15:25:26.362563Z","shell.execute_reply.started":"2025-04-19T15:25:26.358052Z","shell.execute_reply":"2025-04-19T15:25:26.361720Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#Accessing relevant keys\nGroq_key = user_secrets.get_secret(\"Groq\")\nMistral_key = user_secrets.get_secret(\"Mistral\")\n\n# Embedding & vector store settings\nEMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\nVECTOR_STORE_PATH = \"/kaggle/input/cx-agent-dataset\"\n\n# Retrieval settings\nTOP_K = 3\n\n# LLM parameters\nTEMPERATURE = float(0.2)\nMAX_LENGTH = int(512)\nTOP_P = float(0.95)\n\n# Placeholder for vector store instance\nVECTOR_STORE = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:25:28.674509Z","iopub.execute_input":"2025-04-19T15:25:28.674818Z","iopub.status.idle":"2025-04-19T15:25:28.840928Z","shell.execute_reply.started":"2025-04-19T15:25:28.674796Z","shell.execute_reply":"2025-04-19T15:25:28.840030Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# 2. Data source\n\nFor demonstration purposes, we’ve used CSV files to simulate database connections. In a production setting, you can replace these with direct connections to relational databases like PostgreSQL, MySQL, or even SQLite during local development.\n\n1. FAQ: Contains frequently asked questions that don’t require human intervention. These are chunked, embedded using MistralAI embeddings, and stored in ChromaDB for RAG-based retrieval.\n2. Tickets: Stores support tickets created by users for follow-up or escalation.\n3. Payment info: Includes user payment details etc.\n4. User Info: Includes personal data such as room numbers, move-in status, etc.","metadata":{}},{"cell_type":"code","source":"faq_path = '/kaggle/input/cx-agent-dataset/faq.csv'\nticket_path = '/kaggle/input/cx-agent-dataset/tickets.csv'\npayment_path = '/kaggle/input/cx-agent-dataset/tickets.csv'\nuser_info_path = '/kaggle/input/cx-agent-dataset/user_info.csv'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:25:31.573662Z","iopub.execute_input":"2025-04-19T15:25:31.574002Z","iopub.status.idle":"2025-04-19T15:25:31.578587Z","shell.execute_reply.started":"2025-04-19T15:25:31.573973Z","shell.execute_reply":"2025-04-19T15:25:31.577712Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# 3. Agent tools\n\nWe have developed four tools that agent can access to retreive relevant information\n1. FAQ tool : This convert the existing FAQ and user RAG to convert into vector database, any FAQ reletated question can be answered from there\n2. Payment tool : Give user payment information \n3. Ticket tool: Give user ticket information\n4. User info tool: Give user personal information","metadata":{}},{"cell_type":"code","source":"#Faq tool\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import CSVLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_mistralai import MistralAIEmbeddings\n\ndef load_documents(faq_path: str):\n    \"\"\"Load documents from the FAQ CSV file.\"\"\"\n    loader = CSVLoader(file_path=faq_path, source_column=\"question\")\n    return loader.load()\n\n\ndef split_into_chunks(data, chunk_size=150, chunk_overlap=2):\n    \"\"\"Split documents into smaller chunks.\"\"\"\n    splitter = RecursiveCharacterTextSplitter(\n        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap\n    )\n    all_chunks = []\n    for doc in data:\n        chunks = splitter.split_text(doc.page_content)\n        for chunk in chunks:\n            all_chunks.append(doc.__class__(page_content=chunk, metadata=doc.metadata))\n    return all_chunks\n\n\ndef create_vector_store(chunks, Mistral_key):\n    embeddings_model = MistralAIEmbeddings(model=\"mistral-embed\",mistral_api_key=Mistral_key)\n\n    vector_store = Chroma.from_documents(\n        documents=chunks,\n        embedding=embeddings_model,\n        collection_name=\"faq_collection\",\n        persist_directory=None\n    )\n    \n    return vector_store\n\n\ndef get_faq_answer(user_query: str, vector_store, TOP_K) -> str:\n    \"\"\"Retrieve an answer from the FAQ vector store based on the user query.\"\"\"\n    retriever = vector_store.as_retriever(\n        search_type=\"similarity\",\n        search_kwargs={\"k\": TOP_K}\n    )\n    relevant_docs = retriever.get_relevant_documents(user_query)\n    if relevant_docs:\n        return relevant_docs[0].page_content\n    else:\n        return \"I couldn't find an answer to your question in our FAQ database.\"\n\n\ndef initialize_vector_store():\n    \"\"\"Initialize vector store from the FAQ data.\"\"\"\n    documents = load_documents(faq_path)\n    chunks = split_into_chunks(documents)\n    vector_store = create_vector_store(chunks,Mistral_key)\n    return vector_store","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:25:36.075314Z","iopub.execute_input":"2025-04-19T15:25:36.075637Z","iopub.status.idle":"2025-04-19T15:25:37.812608Z","shell.execute_reply.started":"2025-04-19T15:25:36.075613Z","shell.execute_reply":"2025-04-19T15:25:37.811589Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"#Payment_tool\nimport pandas as pd\n\ndef load_payment_data(payment_path):\n    \"\"\"Load payment data from CSV.\"\"\"\n    payment_data = pd.read_csv(payment_path)\n    return payment_data\n\n\ndef get_payment_status(user_id: str, payment_path):\n    \"\"\"\n    Get the latest payment info for a user.\n\n    Args:\n        user_id (str): The user's ID.\n        file_path (str): Path to the payments CSV file.\n\n    Returns:\n        str: A human-readable summary of the payment.\n    \"\"\"\n    df = load_payment_data(payment_path)\n    \n    # Filter and sort by due_date descending (latest first)\n    user_payments = df[df[\"user_id\"] == user_id].sort_values(by=\"due_date\", ascending=False)\n\n    if user_payments.empty:\n        return \"I couldn't find any payment information for your account.\"\n\n    latest_payment = user_payments.iloc[0]\n    \n    summary = (\n        f\"Your last recorded payment is ₹{latest_payment['amount']} \"\n        f\"which was due on {latest_payment['due_date']}. \"\n        f\"{'GST was applied.' if latest_payment['gst_applied'].lower() == 'yes' else 'GST was not applied.'} \"\n        f\"Current payment status: {latest_payment['status']}.\"\n    )\n    \n    return summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:25:44.411324Z","iopub.execute_input":"2025-04-19T15:25:44.411659Z","iopub.status.idle":"2025-04-19T15:25:44.417800Z","shell.execute_reply.started":"2025-04-19T15:25:44.411634Z","shell.execute_reply":"2025-04-19T15:25:44.416937Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"#Ticket tool\nimport pandas as pd\nfrom datetime import datetime\n\ndef load_ticket_data(ticket_path):\n    \"\"\"Load ticket data from CSV.\"\"\"\n    ticket_data = pd.read_csv(ticket_path)\n    return ticket_data\n\ndef get_open_ticket_status(user_id: str, ticket_path):\n    \"\"\"\n    Get the latest open ticket info for a user.\n\n    Args:\n        user_id (str): The user's ID.\n        file_path (str): Path to the tickets CSV file.\n\n    Returns:\n        str: Status of open ticket or a message if none found.\n    \"\"\"\n    df = load_ticket_data(ticket_path)\n    open_tickets = df[(df[\"user_id\"] == user_id) & (df[\"status\"] == \"open\")]\n\n    if open_tickets.empty:\n        return \"You don't have any open support tickets at the moment.\"\n\n    latest_ticket = open_tickets.sort_values(by=\"created_at\", ascending=False).iloc[0]\n    return (\n        f\"Your open ticket (ID: {latest_ticket['ticket_id']}) \"\n        f\"is regarding: '{latest_ticket['issue']}'. It is currently marked as: {latest_ticket['status']}.\"\n    )\n\n\ndef create_ticket(user_id: str, issue: str, ticket_path):\n    \"\"\"\n    Create a new support ticket for a user.\n\n    Args:\n        user_id (str): The user's ID.\n        issue (str): A summary of the unresolved issue.\n        file_path (str): Path to the tickets CSV file.\n\n    Returns:\n        str: Confirmation message with ticket ID.\n    \"\"\"\n    df = load_ticket_data(ticket_path)\n\n    new_ticket = {\n        \"ticket_id\": f\"T{int(datetime.now().timestamp())}\",\n        \"user_id\": user_id,\n        \"issue\": issue,\n        \"status\": \"open\",\n        \"created_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n    }\n\n    df = pd.concat([df, pd.DataFrame([new_ticket])], ignore_index=True)\n    df.to_csv(file_path, index=False)\n\n    return f\"A support ticket has been created for you. Your ticket ID is {new_ticket['ticket_id']}.\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:25:48.022524Z","iopub.execute_input":"2025-04-19T15:25:48.022883Z","iopub.status.idle":"2025-04-19T15:25:48.030942Z","shell.execute_reply.started":"2025-04-19T15:25:48.022858Z","shell.execute_reply":"2025-04-19T15:25:48.030042Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"#User info tool\n\nimport pandas as pd\n\ndef load_user_info(user_info_path):\n    \"\"\"Load user info data from CSV.\"\"\"\n    return pd.read_csv(user_info_path)\n\n\ndef get_user_info(user_id: str, user_info_path):\n    \"\"\"\n    Get user profile info (room, city, check-in date, etc.).\n\n    Args:\n        user_id (str): The user's ID.\n        file_path (str): Path to the user info CSV file.\n\n    Returns:\n        str: A summary of the user's info or an error message.\n    \"\"\"\n    df = load_user_info(user_info_path)\n    user_row = df[df[\"user_id\"] == user_id]\n\n    if user_row.empty:\n        return \"Sorry, I couldn't find your profile information.\"\n\n    user = user_row.iloc[0]\n    return (\n        f\"Here's your profile info:\\n\"\n        f\"Name: {user['name']}\\n\"\n        f\"Room No: {user['room_no']}\\n\"\n        f\"City: {user['city']}\\n\"\n        f\"Check-in Date: {user['checkin_date']}\"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:25:54.511337Z","iopub.execute_input":"2025-04-19T15:25:54.511616Z","iopub.status.idle":"2025-04-19T15:25:54.517879Z","shell.execute_reply.started":"2025-04-19T15:25:54.511597Z","shell.execute_reply":"2025-04-19T15:25:54.516831Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# 4. Langraph Agents nodes \n1. Classify Node: This node classifies the user’s query into predefined categories such as FAQ, ticket, payment, or user info, helping route the request accordingly.\n\n2. Escalation Node: This node determines when to escalate a query, triggering the creation of a support ticket for unresolved or complex issues.\n\n3. Payment Response Node: This node handles queries related to payments by querying the payment database and generating a response based on user details.\n\n4. Ticket Response Node: This node retrieves ticket information for follow-up queries, allowing the system to provide updates or escalate as necessary.\n\n5. User Info Node: This node retrieves and returns user-specific information, such as room numbers or move-in status, from the user info database.\n\n6. FAQ Response Node: This node uses RAG to fetch the most relevant FAQ data, enabling the system to generate accurate and context-aware responses to general queries.\n\n7. LangGraph Node: This node orchestrates the flow of tasks, ensuring the proper handling of responses based on the query classification and user context.","metadata":{}},{"cell_type":"code","source":"#Classifying node\n\nfrom langchain_groq import ChatGroq\nfrom langchain.schema import SystemMessage, HumanMessage\n\ndef classify_node(state: dict) -> dict:\n    \"\"\"\n    Classify user intent using Groq's LLM via LangChain.\n\n    Input state must contain:\n    - 'user_query': the raw query\n    - 'user_id': the user's ID\n\n    Returns updated state with:\n    - 'intent': one of ['faq', 'ticket', 'payment', 'user_info', 'fallback']\n    \"\"\"\n    user_query = state.get(\"user_query\", \"\")\n\n    # Updated prompt for better classification logic\n    system_prompt = SystemMessage(content=\"\"\"\nYou are a classification assistant for a residential support chatbot.\n\nClassify user queries into one of the following:\n\n- faq: If the user is asking about general info, help, or known issues like payment failures, service availability, policies, etc.\n- ticket: Only if the user is **raising a personal complaint or service request** (e.g., \"My Wi-Fi is not working\", \"There is a leak in my bathroom\").\n- payment: If the user asks about rent, GST, amount paid, due date, refunds, etc.\n- user_info: If the user asks about their personal info like room number, check-in, ID details, address, etc.\n- fallback: If you are not sure or the user is just greeting or unclear.\n\nExamples:\nQuery: What time is check-in? → faq  \nQuery: My AC is not working → ticket  \nQuery: When is my rent due? → payment  \nQuery: What is my room number? → user_info  \nQuery: Hello → fallback\n\nOnly respond with one word: faq, ticket, payment, user_info, or fallback.\n\"\"\")\n\n    human_prompt = HumanMessage(content=f\"Query: {user_query}\\nAnswer:\")\n\n    # Use Groq’s LLM\n    llm = ChatGroq(groq_api_key=Groq_key,model_name=\"llama3-70b-8192\")\n\n    response = llm.invoke([system_prompt, human_prompt])\n    intent = response.content.strip().lower()\n\n    return {**state, \"intent\": intent}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:25:58.510512Z","iopub.execute_input":"2025-04-19T15:25:58.510856Z","iopub.status.idle":"2025-04-19T15:25:58.543730Z","shell.execute_reply.started":"2025-04-19T15:25:58.510831Z","shell.execute_reply":"2025-04-19T15:25:58.542725Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"#Escalation node\nfrom langchain_groq import ChatGroq\nfrom langchain.schema import HumanMessage, SystemMessage\n\n\ndef summarize_issue(user_query: str) -> str:\n    \"\"\"Use Groq LLM to summarize user query for ticket description.\"\"\"\n    llm = ChatGroq(groq_api_key=Groq_key, model_name=\"llama3-70b-8192\")\n\n    prompt = f\"Summarize the following issue clearly for creating a support ticket:\\n\\n{user_query}\"\n    response = llm.invoke([\n        SystemMessage(content=\"You are a helpful assistant for writing support ticket summaries.\"),\n        HumanMessage(content=prompt)\n    ])\n    return response.content.strip()\n\n\ndef escalation_node(state: dict) -> dict:\n    \"\"\"\n    Escalates unresolved queries by creating a support ticket.\n    \n    Requires:\n    - 'user_id'\n    - 'user_query'\n    - 'feedback' == 'no'\n    \n    Returns:\n    - 'ticket_id'\n    - 'final_response'\n    - 'escalation_status'\n    \"\"\"\n    user_id = state.get(\"user_id\")\n    user_query = state.get(\"user_query\")\n    feedback = state.get(\"feedback\", \"\").lower()\n\n    if feedback != \"no\":\n        return {**state, \"escalation_status\": \"not_triggered\"}\n\n    #Summarize issue with LLM\n    issue_summary = summarize_issue(user_query)\n\n    #Create ticket using your existing tool\n    ticket_id = create_ticket(user_id, issue_summary, file_path= ticket_path)\n\n    return {\n        **state,\n        \"ticket_id\": ticket_id,\n        \"escalation_status\": \"ticket_created\",\n        \"final_response\": f\"A support ticket has been created for you. Your ticket ID is {ticket_id}.\"\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:26:01.874781Z","iopub.execute_input":"2025-04-19T15:26:01.875088Z","iopub.status.idle":"2025-04-19T15:26:01.882374Z","shell.execute_reply.started":"2025-04-19T15:26:01.875066Z","shell.execute_reply":"2025-04-19T15:26:01.881513Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"#faq node\n\ndef faq_response_node(state: dict) -> dict:\n    \"\"\"\n    Responds to a user query classified as 'faq' using the FAQ vector store.\n\n    Requires:\n    - 'user_query' key in state\n\n    Returns:\n    - Updated state with 'final_response'\n    \"\"\"\n    print(\"✅ Running updated faq_response_node...\")\n\n    user_query = state.get(\"user_query\")\n\n\n    try:\n        # Use top 3 most relevant documents\n        answer = get_faq_answer(user_query, vector_store=VECTOR_STORE, TOP_K=3)\n    except Exception as e:\n        answer = f\"Error while fetching answer from FAQ: {e}\"\n\n    return {\n        **state,\n        \"final_response\": answer\n    }\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:26:05.374552Z","iopub.execute_input":"2025-04-19T15:26:05.375524Z","iopub.status.idle":"2025-04-19T15:26:05.380777Z","shell.execute_reply.started":"2025-04-19T15:26:05.375481Z","shell.execute_reply":"2025-04-19T15:26:05.379794Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"#Payment response node\ndef payment_response_node(state: dict) -> dict:\n    \"\"\"\n    Responds to a user query classified as 'payment' by fetching payment details.\n    \n    Requires:\n    - 'user_id'\n    \n    Returns:\n    - Updated state with 'final_response'\n    \"\"\"\n    user_id = state.get(\"user_id\")\n    answer = get_payment_status(user_id, file_path= payment_path)\n\n    return {\n        **state,\n        \"final_response\": answer\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:26:09.022584Z","iopub.execute_input":"2025-04-19T15:26:09.022926Z","iopub.status.idle":"2025-04-19T15:26:09.029175Z","shell.execute_reply.started":"2025-04-19T15:26:09.022900Z","shell.execute_reply":"2025-04-19T15:26:09.028123Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"#Ticket response node\ndef ticket_response_node(state: dict) -> dict:\n    \"\"\"\n    Responds to a user query classified as 'ticket' by fetching open ticket status.\n    \n    Requires:\n    - 'user_id'\n    \n    Returns:\n    - Updated state with 'final_response'\n    \"\"\"\n    user_id = state.get(\"user_id\")\n    answer = get_open_ticket_status(user_id, file_path= ticket_path)\n\n    return {\n        **state,\n        \"final_response\": answer\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:26:31.917200Z","iopub.execute_input":"2025-04-19T15:26:31.917503Z","iopub.status.idle":"2025-04-19T15:26:31.923061Z","shell.execute_reply.started":"2025-04-19T15:26:31.917480Z","shell.execute_reply":"2025-04-19T15:26:31.922059Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"#User response node\n\ndef user_info_response_node(state: dict) -> dict:\n    \"\"\"\n    Responds to a user query classified as 'user_info' by fetching user profile details.\n    \n    Requires:\n    - 'user_id'\n    \n    Returns:\n    - Updated state with 'final_response'\n    \"\"\"\n    user_id = state.get(\"user_id\")\n    answer = get_user_info(user_id, file_path= user_info_path)\n\n    return {\n        **state,\n        \"final_response\": answer\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:26:39.060608Z","iopub.execute_input":"2025-04-19T15:26:39.061301Z","iopub.status.idle":"2025-04-19T15:26:39.065655Z","shell.execute_reply.started":"2025-04-19T15:26:39.061273Z","shell.execute_reply":"2025-04-19T15:26:39.064818Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"#Langraph flow\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Optional\n\n\n# Define the state schema using TypedDict\nclass AgentState(TypedDict):\n    user_query: str\n    user_id: str\n    intent: str\n    feedback: str\n    ticket_id: Optional[str]\n    escalation_status: Optional[str]\n    final_response: Optional[str]\n\n\n# Define intent-based routing function\ndef intent_router(state: AgentState) -> str:\n    return state.get(\"intent\", \"fallback\")\n\n\n# Define feedback-based routing function\ndef feedback_router(state: AgentState) -> str:\n    return state.get(\"feedback\", \"yes\")\n\n\n# Create LangGraph flow\ngraph = StateGraph(AgentState)\n\n# Add nodes\ngraph.add_node(\"classify\", classify_node)\ngraph.add_node(\"faq_response\", faq_response_node)\ngraph.add_node(\"ticket_response\", ticket_response_node)\ngraph.add_node(\"payment_response\", payment_response_node)\ngraph.add_node(\"user_info_response\", user_info_response_node)\ngraph.add_node(\"escalate\", escalation_node)\n\n# Entry point\ngraph.set_entry_point(\"classify\")\n\n# Conditional routing based on intent\ngraph.add_conditional_edges(\"classify\", intent_router, {\n    \"faq\": \"faq_response\",\n    \"ticket\": \"ticket_response\",\n    \"payment\": \"payment_response\",\n    \"user_info\": \"user_info_response\",\n    \"fallback\": \"escalate\"\n})\n\n# Conditional routing from each response node based on feedback\ngraph.add_conditional_edges(\"faq_response\", feedback_router, {\"no\": \"escalate\",\"yes\": END})\ngraph.add_conditional_edges(\"ticket_response\", feedback_router, {\"no\": \"escalate\",\"yes\": END})\ngraph.add_conditional_edges(\"payment_response\", feedback_router, {\"no\": \"escalate\",\"yes\": END})\ngraph.add_conditional_edges(\"user_info_response\", feedback_router, {\"no\": \"escalate\",\"yes\": END})\n\n# Escalation ends the flow\ngraph.add_edge(\"escalate\", END)\n\n# Compile the graph\ncx_agent_graph = graph.compile()\n\n#Run agent code\ndef run_agent(user_query, user_id, feedback=\"yes\"):\n    initial_state = {\n        \"user_query\": user_query,\n        \"user_id\": user_id,\n        \"feedback\": feedback\n    }\n    final_state = cx_agent_graph.invoke(initial_state)\n    return final_state","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:26:43.080257Z","iopub.execute_input":"2025-04-19T15:26:43.080552Z","iopub.status.idle":"2025-04-19T15:26:43.250507Z","shell.execute_reply.started":"2025-04-19T15:26:43.080531Z","shell.execute_reply":"2025-04-19T15:26:43.249464Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"# 5. Running file","metadata":{}},{"cell_type":"code","source":"# Initialize the vector store\nVECTOR_STORE = initialize_vector_store()\n\ndef main():\n    print(\"=== CX Agent for STANZA LIVING ===\\n\")\n\n    user_id = input(\"Enter your user ID: \").strip()\n    if not user_id:\n        print(\"User ID is required to proceed.\")\n        return\n\n    print(\"\\nYou can type 'exit' anytime to quit.\\n\")\n\n    while True:\n        user_query = input(\"Please mention what you are looking for: \").strip()\n\n        if user_query.lower() == \"exit\":\n            print(\"Goodbye!\")\n            break\n\n        # Call the agent to process the query\n        response = run_agent(user_query=user_query, user_id=user_id)\n\n        print(f\"\\nAgent: {response.get('final_response')}\\n\")\n\n        # Ask for feedback\n        feedback = input(\"Was this issue resolved? (yes/no): \").strip().lower()\n\n        if feedback == \"yes\":\n            print(\"Thank you for your feedback. Have a great day!\")\n            break\n\n        # If issue was not resolved, show ticket if created\n        if response.get(\"escalation_status\") == \"ticket_created\":\n            print(f\"Ticket ID: {response.get('ticket_id')}\\n\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T15:26:49.925979Z","iopub.execute_input":"2025-04-19T15:26:49.926334Z","iopub.status.idle":"2025-04-19T15:27:33.120074Z","shell.execute_reply.started":"2025-04-19T15:26:49.926309Z","shell.execute_reply":"2025-04-19T15:27:33.119330Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/langchain_mistralai/embeddings.py:181: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"=== CX Agent for STANZA LIVING ===\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your user ID:  U101\n"},{"name":"stdout","text":"\nYou can type 'exit' anytime to quit.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Please mention what you are looking for:  How often is housekeeping provided?\n"},{"name":"stdout","text":"✅ Running updated faq_response_node...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1236378595.py:48: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n  relevant_docs = retriever.get_relevant_documents(user_query)\n","output_type":"stream"},{"name":"stdout","text":"\nAgent: question: How often is housekeeping provided?\nanswer: Housekeeping is done once every two days for shared areas and once a week for personal rooms.\n:\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Was this issue resolved? (yes/no):  yes\n"},{"name":"stdout","text":"Thank you for your feedback. Have a great day!\n","output_type":"stream"}],"execution_count":24}]}