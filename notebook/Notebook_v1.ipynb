{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":97258,"sourceType":"competition"},{"sourceId":11388536,"sourceType":"datasetVersion","datasetId":7131662}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:49:19.318869Z","iopub.execute_input":"2025-04-14T01:49:19.319283Z","iopub.status.idle":"2025-04-14T01:49:21.954073Z","shell.execute_reply.started":"2025-04-14T01:49:19.319248Z","shell.execute_reply":"2025-04-14T01:49:21.953220Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cx-agent-dataset/faq.csv\n/kaggle/input/cx-agent-dataset/tickets.csv\n/kaggle/input/cx-agent-dataset/user_info.csv\n/kaggle/input/cx-agent-dataset/payments.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#Libraries \n!pip install -q \\\n  langchain \\\n  langchain-huggingface \\\n  langchain_community \\\n  langchainhub \\\n  langchain-groq \\\n  chromadb \\\n  sentence-transformers \\\n  huggingface-hub \\\n  python-dotenv \\\n  langchain-mistralai \\\n  streamlit\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T01:49:21.955491Z","iopub.execute_input":"2025-04-14T01:49:21.955902Z","iopub.status.idle":"2025-04-14T01:52:11.211568Z","shell.execute_reply.started":"2025-04-14T01:49:21.955877Z","shell.execute_reply":"2025-04-14T01:52:11.210239Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.7/126.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.3/423.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\ngoogle-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -U langgraph","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Accessing relevant keys\nGroq_key = user_secrets.get_secret(\"Groq\")\nMistral_key = user_secrets.get_secret(\"Mistral\")\n\n# Embedding & vector store settings\nEMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\nVECTOR_STORE_PATH = \"/kaggle/input/cx-agent-dataset\"\n\n# Retrieval settings\nTOP_K = 3\n\n# LLM parameters\nTEMPERATURE = float(0.2)\nMAX_LENGTH = int(512)\nTOP_P = float(0.95)\n\n# Placeholder for vector store instance\nVECTOR_STORE = None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data source","metadata":{}},{"cell_type":"code","source":"faq_path = '/kaggle/input/cx-agent-dataset/faq.csv'\nticket_path = '/kaggle/input/cx-agent-dataset/tickets.csv'\npayment_path = '/kaggle/input/cx-agent-dataset/tickets.csv'\nuser_info_path = '/kaggle/input/cx-agent-dataset/user_info.csv'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1.Tools","metadata":{}},{"cell_type":"markdown","source":"# # 1(a) - FAQ Tool\n# # 1(a) - Payment Tool\n# # 1(a) - Ticket Tool\n# # 1(a) - User info Tool","metadata":{}},{"cell_type":"code","source":"#Faq tool\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import CSVLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_mistralai import MistralAIEmbeddings\n\ndef load_documents(faq_path: str):\n    \"\"\"Load documents from the FAQ CSV file.\"\"\"\n    loader = CSVLoader(file_path=faq_path, source_column=\"question\")\n    return loader.load()\n\n\ndef split_into_chunks(data, chunk_size=150, chunk_overlap=2):\n    \"\"\"Split documents into smaller chunks.\"\"\"\n    splitter = RecursiveCharacterTextSplitter(\n        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap\n    )\n    all_chunks = []\n    for doc in data:\n        chunks = splitter.split_text(doc.page_content)\n        for chunk in chunks:\n            all_chunks.append(doc.__class__(page_content=chunk, metadata=doc.metadata))\n    return all_chunks\n\n\ndef create_vector_store(chunks, Mistral_key):\n    embeddings_model = MistralAIEmbeddings(model=\"mistral-embed\",mistral_api_key=Mistral_key)\n\n    vector_store = Chroma.from_documents(\n        documents=chunks,\n        embedding=embeddings_model,\n        collection_name=\"faq_collection\",\n        persist_directory=None\n    )\n    \n    return vector_store\n\n\ndef get_faq_answer(user_query: str, vector_store, TOP_K) -> str:\n    \"\"\"Retrieve an answer from the FAQ vector store based on the user query.\"\"\"\n    retriever = vector_store.as_retriever(\n        search_type=\"similarity\",\n        search_kwargs={\"k\": TOP_K}\n    )\n    relevant_docs = retriever.get_relevant_documents(user_query)\n    if relevant_docs:\n        return relevant_docs[0].page_content\n    else:\n        return \"I couldn't find an answer to your question in our FAQ database.\"\n\n\ndef initialize_vector_store():\n    \"\"\"Initialize vector store from the FAQ data.\"\"\"\n    documents = load_documents(faq_path)\n    chunks = split_into_chunks(documents)\n    vector_store = create_vector_store(chunks,Mistral_key)\n    return vector_store","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Payment_tool\nimport pandas as pd\n\ndef load_payment_data(payment_path):\n    \"\"\"Load payment data from CSV.\"\"\"\n    payment_data = pd.read_csv(payment_path)\n    return payment_data\n\n\ndef get_payment_status(user_id: str, payment_path):\n    \"\"\"\n    Get the latest payment info for a user.\n\n    Args:\n        user_id (str): The user's ID.\n        file_path (str): Path to the payments CSV file.\n\n    Returns:\n        str: A human-readable summary of the payment.\n    \"\"\"\n    df = load_payment_data(payment_path)\n    \n    # Filter and sort by due_date descending (latest first)\n    user_payments = df[df[\"user_id\"] == user_id].sort_values(by=\"due_date\", ascending=False)\n\n    if user_payments.empty:\n        return \"I couldn't find any payment information for your account.\"\n\n    latest_payment = user_payments.iloc[0]\n    \n    summary = (\n        f\"Your last recorded payment is ₹{latest_payment['amount']} \"\n        f\"which was due on {latest_payment['due_date']}. \"\n        f\"{'GST was applied.' if latest_payment['gst_applied'].lower() == 'yes' else 'GST was not applied.'} \"\n        f\"Current payment status: {latest_payment['status']}.\"\n    )\n    \n    return summary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Ticket tool\nimport pandas as pd\nfrom datetime import datetime\n\ndef load_ticket_data(ticket_path):\n    \"\"\"Load ticket data from CSV.\"\"\"\n    ticket_data = pd.read_csv(ticket_path)\n    return ticket_data\n\ndef get_open_ticket_status(user_id: str, ticket_path):\n    \"\"\"\n    Get the latest open ticket info for a user.\n\n    Args:\n        user_id (str): The user's ID.\n        file_path (str): Path to the tickets CSV file.\n\n    Returns:\n        str: Status of open ticket or a message if none found.\n    \"\"\"\n    df = load_ticket_data(ticket_path)\n    open_tickets = df[(df[\"user_id\"] == user_id) & (df[\"status\"] == \"open\")]\n\n    if open_tickets.empty:\n        return \"You don't have any open support tickets at the moment.\"\n\n    latest_ticket = open_tickets.sort_values(by=\"created_at\", ascending=False).iloc[0]\n    return (\n        f\"Your open ticket (ID: {latest_ticket['ticket_id']}) \"\n        f\"is regarding: '{latest_ticket['issue']}'. It is currently marked as: {latest_ticket['status']}.\"\n    )\n\n\ndef create_ticket(user_id: str, issue: str, ticket_path):\n    \"\"\"\n    Create a new support ticket for a user.\n\n    Args:\n        user_id (str): The user's ID.\n        issue (str): A summary of the unresolved issue.\n        file_path (str): Path to the tickets CSV file.\n\n    Returns:\n        str: Confirmation message with ticket ID.\n    \"\"\"\n    df = load_ticket_data(ticket_path)\n\n    new_ticket = {\n        \"ticket_id\": f\"T{int(datetime.now().timestamp())}\",\n        \"user_id\": user_id,\n        \"issue\": issue,\n        \"status\": \"open\",\n        \"created_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n    }\n\n    df = pd.concat([df, pd.DataFrame([new_ticket])], ignore_index=True)\n    df.to_csv(file_path, index=False)\n\n    return f\"A support ticket has been created for you. Your ticket ID is {new_ticket['ticket_id']}.\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#User info tool\n\nimport pandas as pd\n\ndef load_user_info(user_info_path):\n    \"\"\"Load user info data from CSV.\"\"\"\n    return pd.read_csv(user_info_path)\n\n\ndef get_user_info(user_id: str, user_info_path):\n    \"\"\"\n    Get user profile info (room, city, check-in date, etc.).\n\n    Args:\n        user_id (str): The user's ID.\n        file_path (str): Path to the user info CSV file.\n\n    Returns:\n        str: A summary of the user's info or an error message.\n    \"\"\"\n    df = load_user_info(user_info_path)\n    user_row = df[df[\"user_id\"] == user_id]\n\n    if user_row.empty:\n        return \"Sorry, I couldn't find your profile information.\"\n\n    user = user_row.iloc[0]\n    return (\n        f\"Here's your profile info:\\n\"\n        f\"Name: {user['name']}\\n\"\n        f\"Room No: {user['room_no']}\\n\"\n        f\"City: {user['city']}\\n\"\n        f\"Check-in Date: {user['checkin_date']}\"\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Agents\n# # 4 (a). Classify node  \n# # 4 (b). Escalation node   \n# # 4 (c). Langgraph node  \n# # 4 (d). Payment response node  \n# # 4 (e). Ticket response node  \n# # 4 (f). User info node  \n# # 4 (g). FAQ Response node ","metadata":{}},{"cell_type":"code","source":"#Classifying node\n\nfrom langchain_groq import ChatGroq\nfrom langchain.schema import SystemMessage, HumanMessage\n\ndef classify_node(state: dict) -> dict:\n    \"\"\"\n    Classify user intent using Groq's LLM via LangChain.\n\n    Input state must contain:\n    - 'user_query': the raw query\n    - 'user_id': the user's ID\n\n    Returns updated state with:\n    - 'intent': one of ['faq', 'ticket', 'payment', 'user_info', 'fallback']\n    \"\"\"\n    user_query = state.get(\"user_query\", \"\")\n\n    # Updated prompt for better classification logic\n    system_prompt = SystemMessage(content=\"\"\"\nYou are a classification assistant for a residential support chatbot.\n\nClassify user queries into one of the following:\n\n- faq: If the user is asking about general info, help, or known issues like payment failures, service availability, policies, etc.\n- ticket: Only if the user is **raising a personal complaint or service request** (e.g., \"My Wi-Fi is not working\", \"There is a leak in my bathroom\").\n- payment: If the user asks about rent, GST, amount paid, due date, refunds, etc.\n- user_info: If the user asks about their personal info like room number, check-in, ID details, address, etc.\n- fallback: If you are not sure or the user is just greeting or unclear.\n\nExamples:\nQuery: What time is check-in? → faq  \nQuery: My AC is not working → ticket  \nQuery: When is my rent due? → payment  \nQuery: What is my room number? → user_info  \nQuery: Hello → fallback\n\nOnly respond with one word: faq, ticket, payment, user_info, or fallback.\n\"\"\")\n\n    human_prompt = HumanMessage(content=f\"Query: {user_query}\\nAnswer:\")\n\n    # Use Groq’s LLM\n    llm = ChatGroq(groq_api_key=Groq_key,model_name=\"llama3-70b-8192\")\n\n    response = llm.invoke([system_prompt, human_prompt])\n    intent = response.content.strip().lower()\n\n    return {**state, \"intent\": intent}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Escalation node\nfrom langchain_groq import ChatGroq\nfrom langchain.schema import HumanMessage, SystemMessage\n\n\ndef summarize_issue(user_query: str) -> str:\n    \"\"\"Use Groq LLM to summarize user query for ticket description.\"\"\"\n    llm = ChatGroq(groq_api_key=Groq_key, model_name=\"llama3-70b-8192\")\n\n    prompt = f\"Summarize the following issue clearly for creating a support ticket:\\n\\n{user_query}\"\n    response = llm.invoke([\n        SystemMessage(content=\"You are a helpful assistant for writing support ticket summaries.\"),\n        HumanMessage(content=prompt)\n    ])\n    return response.content.strip()\n\n\ndef escalation_node(state: dict) -> dict:\n    \"\"\"\n    Escalates unresolved queries by creating a support ticket.\n    \n    Requires:\n    - 'user_id'\n    - 'user_query'\n    - 'feedback' == 'no'\n    \n    Returns:\n    - 'ticket_id'\n    - 'final_response'\n    - 'escalation_status'\n    \"\"\"\n    user_id = state.get(\"user_id\")\n    user_query = state.get(\"user_query\")\n    feedback = state.get(\"feedback\", \"\").lower()\n\n    if feedback != \"no\":\n        return {**state, \"escalation_status\": \"not_triggered\"}\n\n    #Summarize issue with LLM\n    issue_summary = summarize_issue(user_query)\n\n    #Create ticket using your existing tool\n    ticket_id = create_ticket(user_id, issue_summary, file_path= ticket_path)\n\n    return {\n        **state,\n        \"ticket_id\": ticket_id,\n        \"escalation_status\": \"ticket_created\",\n        \"final_response\": f\"A support ticket has been created for you. Your ticket ID is {ticket_id}.\"\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#faq node\n\ndef faq_response_node(state: dict) -> dict:\n    \"\"\"\n    Responds to a user query classified as 'faq' using the FAQ vector store.\n\n    Requires:\n    - 'user_query' key in state\n\n    Returns:\n    - Updated state with 'final_response'\n    \"\"\"\n    print(\"✅ Running updated faq_response_node...\")\n\n    user_query = state.get(\"user_query\")\n\n\n    try:\n        # Use top 3 most relevant documents\n        answer = get_faq_answer(user_query, vector_store=VECTOR_STORE, TOP_K=3)\n    except Exception as e:\n        answer = f\"Error while fetching answer from FAQ: {e}\"\n\n    return {\n        **state,\n        \"final_response\": answer\n    }\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Payment response node\ndef payment_response_node(state: dict) -> dict:\n    \"\"\"\n    Responds to a user query classified as 'payment' by fetching payment details.\n    \n    Requires:\n    - 'user_id'\n    \n    Returns:\n    - Updated state with 'final_response'\n    \"\"\"\n    user_id = state.get(\"user_id\")\n    answer = get_payment_status(user_id, file_path= payment_path)\n\n    return {\n        **state,\n        \"final_response\": answer\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Ticket response node\ndef ticket_response_node(state: dict) -> dict:\n    \"\"\"\n    Responds to a user query classified as 'ticket' by fetching open ticket status.\n    \n    Requires:\n    - 'user_id'\n    \n    Returns:\n    - Updated state with 'final_response'\n    \"\"\"\n    user_id = state.get(\"user_id\")\n    answer = get_open_ticket_status(user_id, file_path= ticket_path)\n\n    return {\n        **state,\n        \"final_response\": answer\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#User response node\n\ndef user_info_response_node(state: dict) -> dict:\n    \"\"\"\n    Responds to a user query classified as 'user_info' by fetching user profile details.\n    \n    Requires:\n    - 'user_id'\n    \n    Returns:\n    - Updated state with 'final_response'\n    \"\"\"\n    user_id = state.get(\"user_id\")\n    answer = get_user_info(user_id, file_path= user_info_path)\n\n    return {\n        **state,\n        \"final_response\": answer\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Langraph flow\nfrom langgraph.graph import StateGraph, END\nfrom typing import TypedDict, Optional\n\n\n# Define the state schema using TypedDict\nclass AgentState(TypedDict):\n    user_query: str\n    user_id: str\n    intent: str\n    feedback: str\n    ticket_id: Optional[str]\n    escalation_status: Optional[str]\n    final_response: Optional[str]\n\n\n# Define intent-based routing function\ndef intent_router(state: AgentState) -> str:\n    return state.get(\"intent\", \"fallback\")\n\n\n# Define feedback-based routing function\ndef feedback_router(state: AgentState) -> str:\n    return state.get(\"feedback\", \"yes\")\n\n\n# Create LangGraph flow\ngraph = StateGraph(AgentState)\n\n# Add nodes\ngraph.add_node(\"classify\", classify_node)\ngraph.add_node(\"faq_response\", faq_response_node)\ngraph.add_node(\"ticket_response\", ticket_response_node)\ngraph.add_node(\"payment_response\", payment_response_node)\ngraph.add_node(\"user_info_response\", user_info_response_node)\ngraph.add_node(\"escalate\", escalation_node)\n\n# Entry point\ngraph.set_entry_point(\"classify\")\n\n# Conditional routing based on intent\ngraph.add_conditional_edges(\"classify\", intent_router, {\n    \"faq\": \"faq_response\",\n    \"ticket\": \"ticket_response\",\n    \"payment\": \"payment_response\",\n    \"user_info\": \"user_info_response\",\n    \"fallback\": \"escalate\"\n})\n\n# Conditional routing from each response node based on feedback\ngraph.add_conditional_edges(\"faq_response\", feedback_router, {\"no\": \"escalate\",\"yes\": END})\ngraph.add_conditional_edges(\"ticket_response\", feedback_router, {\"no\": \"escalate\",\"yes\": END})\ngraph.add_conditional_edges(\"payment_response\", feedback_router, {\"no\": \"escalate\",\"yes\": END})\ngraph.add_conditional_edges(\"user_info_response\", feedback_router, {\"no\": \"escalate\",\"yes\": END})\n\n# Escalation ends the flow\ngraph.add_edge(\"escalate\", END)\n\n# Compile the graph\ncx_agent_graph = graph.compile()\n\n#Run agent code\ndef run_agent(user_query, user_id, feedback=\"yes\"):\n    initial_state = {\n        \"user_query\": user_query,\n        \"user_id\": user_id,\n        \"feedback\": feedback\n    }\n    final_state = cx_agent_graph.invoke(initial_state)\n    return final_state","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Running file","metadata":{}},{"cell_type":"code","source":"# Initialize the vector store\nVECTOR_STORE = initialize_vector_store()\n\ndef main():\n    print(\"=== CX Agent for STANZA LIVING ===\\n\")\n\n    user_id = input(\"Enter your user ID: \").strip()\n    if not user_id:\n        print(\"User ID is required to proceed.\")\n        return\n\n    print(\"\\nYou can type 'exit' anytime to quit.\\n\")\n\n    while True:\n        user_query = input(\"Please mention what you are looking for: \").strip()\n\n        if user_query.lower() == \"exit\":\n            print(\"Goodbye!\")\n            break\n\n        # Call the agent to process the query\n        response = run_agent(user_query=user_query, user_id=user_id)\n\n        print(f\"\\nAgent: {response.get('final_response')}\\n\")\n\n        # Ask for feedback\n        feedback = input(\"Was this issue resolved? (yes/no): \").strip().lower()\n\n        if feedback == \"yes\":\n            print(\"Thank you for your feedback. Have a great day!\")\n            break\n\n        # If issue was not resolved, show ticket if created\n        if response.get(\"escalation_status\") == \"ticket_created\":\n            print(f\"Ticket ID: {response.get('ticket_id')}\\n\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}